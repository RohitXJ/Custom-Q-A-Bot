{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "1d033724",
   "metadata": {},
   "source": [
    "# Imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "f75f8930",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import re\n",
    "import numpy as np\n",
    "import faiss\n",
    "from sentence_transformers import SentenceTransformer\n",
    "from transformers import AutoModelForCausalLM, AutoTokenizer\n",
    "import torch"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "156f9946",
   "metadata": {},
   "source": [
    "# Data Loading"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "fa8ba32a",
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_txt(file_path):\n",
    "    with open(file_path, 'r', encoding='utf-8') as f:\n",
    "        text = f.read()\n",
    "    return text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "637a0591",
   "metadata": {},
   "outputs": [],
   "source": [
    "def clean_text(text):\n",
    "    # Remove extra newlines and spaces\n",
    "    text = re.sub(r'\\n+', '\\n', text)\n",
    "    text = re.sub(r'[ \\t]+', ' ', text)\n",
    "    return text.strip()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "ac2d1ab3",
   "metadata": {},
   "outputs": [],
   "source": [
    "def chunk_text(text, max_tokens=100):\n",
    "    \"\"\"\n",
    "    Splits text into chunks of ~max_tokens (approx. by word count for simplicity).\n",
    "    In real scenarios, you'd use a tokenizer for precise token counts.\n",
    "    \"\"\"\n",
    "    words = text.split()\n",
    "    chunks = []\n",
    "    for i in range(0, len(words), max_tokens):\n",
    "        chunk = ' '.join(words[i:i + max_tokens])\n",
    "        chunks.append(chunk)\n",
    "    return chunks"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "18bb987a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "âœ… Loaded 43 chunks.\n",
      "ðŸ”¹ First chunk preview:\n",
      " Hello! This is a sample text document created to test your Language Model project. As an AI assistant, I can help you manage your schedule and remind you of any upcoming meetings or tasks you have. Do you have any meetings today? Or perhaps you need help organizing your to-do list? If you have work to do, it's important to prioritize tasks based on deadlines and importance. I can help you create reminders for your work, so you never miss a deadline. Talking about preferences, do you like a particular type of cuisine or food? For example, I enjoy discussing\n"
     ]
    }
   ],
   "source": [
    "file_path = \"sample.txt\" \n",
    "text = load_txt(file_path)\n",
    "text = clean_text(text)\n",
    "chunks = chunk_text(text)\n",
    "\n",
    "print(f\"âœ… Loaded {len(chunks)} chunks.\")\n",
    "print(\"ðŸ”¹ First chunk preview:\\n\", chunks[0])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "af1700f1",
   "metadata": {},
   "source": [
    "# Generate Embeddings + Build Vector DB with FAISS"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "548d9ccb",
   "metadata": {},
   "outputs": [],
   "source": [
    "model_embedd = SentenceTransformer('all-MiniLM-L6-v2')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "65a3d1ce",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "98a7f78d1270401a8aad3f1ddf4f8001",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Batches:   0%|          | 0/2 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "f:\\AI ENGINEER LEVEL MAP\\HIGH LEVEL\\Custom-Q-A-Bot-\\llm_rag_env\\Lib\\site-packages\\torch\\nn\\modules\\module.py:1762: FutureWarning: `encoder_attention_mask` is deprecated and will be removed in version 4.55.0 for `BertSdpaSelfAttention.forward`.\n",
      "  return forward_call(*args, **kwargs)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Shape of one embedding: (384,)\n"
     ]
    }
   ],
   "source": [
    "# Convert each chunk into a vector\n",
    "embeddings = model_embedd.encode(chunks, show_progress_bar=True)\n",
    "\n",
    "# Check one embedding\n",
    "print(f\"Shape of one embedding: {embeddings[0].shape}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "7ab15e7c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "âœ… Added 43 vectors to the FAISS index.\n"
     ]
    }
   ],
   "source": [
    "# Each vector has 384 dimensions for MiniLM model\n",
    "dimension = embeddings[0].shape[0]\n",
    "index = faiss.IndexFlatL2(dimension)  # L2 = Euclidean distance\n",
    "\n",
    "# Convert to numpy array and add to index\n",
    "embedding_matrix = np.array(embeddings).astype('float32')\n",
    "index.add(embedding_matrix)\n",
    "\n",
    "print(f\"âœ… Added {index.ntotal} vectors to the FAISS index.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "876f27fd",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Simple Python list can work for now\n",
    "chunk_id_to_text = {i: chunk for i, chunk in enumerate(chunks)}"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "14c29739",
   "metadata": {},
   "source": [
    "# Semantic Search + LLM Answering"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "50a93659",
   "metadata": {},
   "outputs": [],
   "source": [
    "# âœ… Step 4a: Embed the question\n",
    "def embed_query(question, model):\n",
    "    return model.encode([question])[0]  # Single vector"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "38589394",
   "metadata": {},
   "outputs": [],
   "source": [
    "# âœ… Step 4b: Use FAISS to find similar chunks\n",
    "def search_faiss(query_vector, index, top_k=3):\n",
    "    query_vector = np.array(query_vector).astype('float32').reshape(1, -1)\n",
    "    distances, indices = index.search(query_vector, top_k)\n",
    "    return indices[0]  # Top-k chunk indices"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "54c8f25a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# âœ… Step 4c: Build the prompt for the LLM\n",
    "def build_prompt(question, chunk_indices, chunk_map):\n",
    "    prompt = \"You are an AI assistant. Use the following context to answer the question.\\n\\nContext:\\n\"\n",
    "    for idx in chunk_indices:\n",
    "        prompt += chunk_map[idx] + \"\\n\"\n",
    "    prompt += f\"\\nQuestion: {question}\\nAnswer:\"\n",
    "    return prompt"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9a8f05fe",
   "metadata": {},
   "source": [
    "# Load the LLM and Generate a Response"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "88a93f20",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "8285f591e6d34bacb15f55932a8b4090",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Loading checkpoint shards:   0%|          | 0/2 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "PhiForCausalLM(\n",
       "  (model): PhiModel(\n",
       "    (embed_tokens): Embedding(51200, 2560)\n",
       "    (layers): ModuleList(\n",
       "      (0-31): 32 x PhiDecoderLayer(\n",
       "        (self_attn): PhiAttention(\n",
       "          (q_proj): Linear(in_features=2560, out_features=2560, bias=True)\n",
       "          (k_proj): Linear(in_features=2560, out_features=2560, bias=True)\n",
       "          (v_proj): Linear(in_features=2560, out_features=2560, bias=True)\n",
       "          (dense): Linear(in_features=2560, out_features=2560, bias=True)\n",
       "        )\n",
       "        (mlp): PhiMLP(\n",
       "          (activation_fn): NewGELUActivation()\n",
       "          (fc1): Linear(in_features=2560, out_features=10240, bias=True)\n",
       "          (fc2): Linear(in_features=10240, out_features=2560, bias=True)\n",
       "        )\n",
       "        (input_layernorm): LayerNorm((2560,), eps=1e-05, elementwise_affine=True)\n",
       "        (resid_dropout): Dropout(p=0.1, inplace=False)\n",
       "      )\n",
       "    )\n",
       "    (rotary_emb): PhiRotaryEmbedding()\n",
       "    (embed_dropout): Dropout(p=0.0, inplace=False)\n",
       "    (final_layernorm): LayerNorm((2560,), eps=1e-05, elementwise_affine=True)\n",
       "  )\n",
       "  (lm_head): Linear(in_features=2560, out_features=51200, bias=True)\n",
       ")"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# âœ… Load the Phi-2 model\n",
    "model_id = \"microsoft/phi-2\"\n",
    "device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
    "\n",
    "tokenizer = AutoTokenizer.from_pretrained(model_id)\n",
    "model = AutoModelForCausalLM.from_pretrained(model_id, torch_dtype=torch.float16 if device==\"cuda\" else torch.float32)\n",
    "model.to(device)\n",
    "model.eval()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "5b635bc4",
   "metadata": {},
   "outputs": [],
   "source": [
    "# âœ… Generate answer from prompt\n",
    "def generate_answer(prompt, max_new_tokens=150):\n",
    "    inputs = tokenizer(prompt, return_tensors=\"pt\").to(device)\n",
    "    with torch.no_grad():\n",
    "        outputs = model.generate(\n",
    "            **inputs,\n",
    "            max_new_tokens=max_new_tokens,\n",
    "            do_sample=True,\n",
    "            temperature=0.7,\n",
    "            top_k=50,\n",
    "            top_p=0.9\n",
    "        )\n",
    "    answer = tokenizer.decode(outputs[0], skip_special_tokens=True)\n",
    "    return answer[len(prompt):].strip()  # Remove the prompt part from output\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "acff2ac9",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ðŸ¤– LLM Answer:\n",
      " Yes, I can help you manage your schedule and remind you of any upcoming meetings or tasks you have.\n"
     ]
    }
   ],
   "source": [
    "# ðŸ” Sample question\n",
    "question = \"Can you help me with scheduling meetings?\"\n",
    "\n",
    "# ðŸ”Ž Embed + search + build prompt\n",
    "query_vector = embed_query(question, model_embedd)\n",
    "top_indices = search_faiss(query_vector, index, top_k=3)\n",
    "final_prompt = build_prompt(question, top_indices, chunk_id_to_text)\n",
    "\n",
    "# ðŸ’¬ Generate LLM response\n",
    "answer = generate_answer(final_prompt)\n",
    "print(\"ðŸ¤– LLM Answer:\\n\", answer)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4fbf336b",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "llm_rag_env",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
