{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "1d033724",
   "metadata": {},
   "source": [
    "# Imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "f75f8930",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import re\n",
    "import numpy as np\n",
    "import faiss\n",
    "from sentence_transformers import SentenceTransformer\n",
    "from transformers import AutoModelForCausalLM, AutoTokenizer\n",
    "import torch"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "156f9946",
   "metadata": {},
   "source": [
    "# Data Loading"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "fa8ba32a",
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_txt(file_path):\n",
    "    with open(file_path, 'r', encoding='utf-8') as f:\n",
    "        text = f.read()\n",
    "    return text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "637a0591",
   "metadata": {},
   "outputs": [],
   "source": [
    "def clean_text(text):\n",
    "    # Remove extra newlines and spaces\n",
    "    text = re.sub(r'\\n+', '\\n', text)\n",
    "    text = re.sub(r'[ \\t]+', ' ', text)\n",
    "    return text.strip()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "ac2d1ab3",
   "metadata": {},
   "outputs": [],
   "source": [
    "def chunk_text(text, max_tokens=100):\n",
    "    \"\"\"\n",
    "    Splits text into chunks of ~max_tokens (approx. by word count for simplicity).\n",
    "    In real scenarios, you'd use a tokenizer for precise token counts.\n",
    "    \"\"\"\n",
    "    words = text.split()\n",
    "    chunks = []\n",
    "    for i in range(0, len(words), max_tokens):\n",
    "        chunk = ' '.join(words[i:i + max_tokens])\n",
    "        chunks.append(chunk)\n",
    "    return chunks"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "18bb987a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "âœ… Loaded 43 chunks.\n",
      "ðŸ”¹ First chunk preview:\n",
      " Hello! This is a sample text document created to test your Language Model project. As an AI assistant, I can help you manage your schedule and remind you of any upcoming meetings or tasks you have. Do you have any meetings today? Or perhaps you need help organizing your to-do list? If you have work to do, it's important to prioritize tasks based on deadlines and importance. I can help you create reminders for your work, so you never miss a deadline. Talking about preferences, do you like a particular type of cuisine or food? For example, I enjoy discussing\n"
     ]
    }
   ],
   "source": [
    "file_path = \"sample.txt\" \n",
    "text = load_txt(file_path)\n",
    "text = clean_text(text)\n",
    "chunks = chunk_text(text)\n",
    "\n",
    "print(f\"âœ… Loaded {len(chunks)} chunks.\")\n",
    "print(\"ðŸ”¹ First chunk preview:\\n\", chunks[0])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "af1700f1",
   "metadata": {},
   "source": [
    "# Generate Embeddings + Build Vector DB with FAISS"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "548d9ccb",
   "metadata": {},
   "outputs": [],
   "source": [
    "model = SentenceTransformer('all-MiniLM-L6-v2')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "65a3d1ce",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "8e9ce7237f8540c794c36a0f86483628",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Batches:   0%|          | 0/2 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "f:\\AI ENGINEER LEVEL MAP\\HIGH LEVEL\\Custom-Q-A-Bot-\\llm_rag_env\\Lib\\site-packages\\torch\\nn\\modules\\module.py:1762: FutureWarning: `encoder_attention_mask` is deprecated and will be removed in version 4.55.0 for `BertSdpaSelfAttention.forward`.\n",
      "  return forward_call(*args, **kwargs)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Shape of one embedding: (384,)\n"
     ]
    }
   ],
   "source": [
    "# Convert each chunk into a vector\n",
    "embeddings = model.encode(chunks, show_progress_bar=True)\n",
    "\n",
    "# Check one embedding\n",
    "print(f\"Shape of one embedding: {embeddings[0].shape}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "7ab15e7c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "âœ… Added 43 vectors to the FAISS index.\n"
     ]
    }
   ],
   "source": [
    "# Each vector has 384 dimensions for MiniLM model\n",
    "dimension = embeddings[0].shape[0]\n",
    "index = faiss.IndexFlatL2(dimension)  # L2 = Euclidean distance\n",
    "\n",
    "# Convert to numpy array and add to index\n",
    "embedding_matrix = np.array(embeddings).astype('float32')\n",
    "index.add(embedding_matrix)\n",
    "\n",
    "print(f\"âœ… Added {index.ntotal} vectors to the FAISS index.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "876f27fd",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Simple Python list can work for now\n",
    "chunk_id_to_text = {i: chunk for i, chunk in enumerate(chunks)}"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "14c29739",
   "metadata": {},
   "source": [
    "# Semantic Search + LLM Answering"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "50a93659",
   "metadata": {},
   "outputs": [],
   "source": [
    "# âœ… Step 4a: Embed the question\n",
    "def embed_query(question, model):\n",
    "    return model.encode([question])[0]  # Single vector"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "38589394",
   "metadata": {},
   "outputs": [],
   "source": [
    "# âœ… Step 4b: Use FAISS to find similar chunks\n",
    "def search_faiss(query_vector, index, top_k=3):\n",
    "    query_vector = np.array(query_vector).astype('float32').reshape(1, -1)\n",
    "    distances, indices = index.search(query_vector, top_k)\n",
    "    return indices[0]  # Top-k chunk indices"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "54c8f25a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# âœ… Step 4c: Build the prompt for the LLM\n",
    "def build_prompt(question, chunk_indices, chunk_map):\n",
    "    prompt = \"You are an AI assistant. Use the following context to answer the question.\\n\\nContext:\\n\"\n",
    "    for idx in chunk_indices:\n",
    "        prompt += chunk_map[idx] + \"\\n\"\n",
    "    prompt += f\"\\nQuestion: {question}\\nAnswer:\"\n",
    "    return prompt"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9a8f05fe",
   "metadata": {},
   "source": [
    "# Load the LLM and Generate a Response"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "88a93f20",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "be400beb2dae43f59594813bab913261",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "tokenizer_config.json: 0.00B [00:00, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "f:\\AI ENGINEER LEVEL MAP\\HIGH LEVEL\\Custom-Q-A-Bot-\\llm_rag_env\\Lib\\site-packages\\huggingface_hub\\file_download.py:143: UserWarning: `huggingface_hub` cache-system uses symlinks by default to efficiently store duplicated files but your machine does not support them in C:\\Users\\RJ\\.cache\\huggingface\\hub\\models--microsoft--phi-2. Caching files will still work but in a degraded version that might require more space on your disk. This warning can be disabled by setting the `HF_HUB_DISABLE_SYMLINKS_WARNING` environment variable. For more details, see https://huggingface.co/docs/huggingface_hub/how-to-cache#limitations.\n",
      "To support symlinks on Windows, you either need to activate Developer Mode or to run Python as an administrator. In order to activate developer mode, see this article: https://docs.microsoft.com/en-us/windows/apps/get-started/enable-your-device-for-development\n",
      "  warnings.warn(message)\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "1a304a71de5a47e6b3038763a484485a",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "vocab.json: 0.00B [00:00, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "919f5ed725b642dda9311cc510154217",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "merges.txt: 0.00B [00:00, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "dba12c3fcb714923812bda95bc0a125e",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "tokenizer.json: 0.00B [00:00, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "273c0e7ba4024817ade5b13e936d06ba",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "added_tokens.json: 0.00B [00:00, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "5d39022026b44b61aafbf3ec2678ab50",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "special_tokens_map.json:   0%|          | 0.00/99.0 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "5dee06cb38a240fd89245c3b3dfd7f10",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "config.json:   0%|          | 0.00/735 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "77613a788c8e4d3b82c58db29ecbaff2",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "model.safetensors.index.json: 0.00B [00:00, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "fe29e4acaf094289833ad996938fefda",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Fetching 2 files:   0%|          | 0/2 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Xet Storage is enabled for this repo, but the 'hf_xet' package is not installed. Falling back to regular HTTP download. For better performance, install the package with: `pip install huggingface_hub[hf_xet]` or `pip install hf_xet`\n",
      "Xet Storage is enabled for this repo, but the 'hf_xet' package is not installed. Falling back to regular HTTP download. For better performance, install the package with: `pip install huggingface_hub[hf_xet]` or `pip install hf_xet`\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "f8f91a448d9b437188346c3f5f73d494",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "model-00002-of-00002.safetensors:   0%|          | 0.00/564M [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "f31c2b02624d465088f289cbd703d9c7",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "model-00001-of-00002.safetensors:   0%|          | 0.00/5.00G [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "bcd731438e164f469666e0eda16e17ad",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Loading checkpoint shards:   0%|          | 0/2 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "4b0b30ddf94f44608b58ec4eee3a91c5",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "generation_config.json:   0%|          | 0.00/124 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "PhiForCausalLM(\n",
       "  (model): PhiModel(\n",
       "    (embed_tokens): Embedding(51200, 2560)\n",
       "    (layers): ModuleList(\n",
       "      (0-31): 32 x PhiDecoderLayer(\n",
       "        (self_attn): PhiAttention(\n",
       "          (q_proj): Linear(in_features=2560, out_features=2560, bias=True)\n",
       "          (k_proj): Linear(in_features=2560, out_features=2560, bias=True)\n",
       "          (v_proj): Linear(in_features=2560, out_features=2560, bias=True)\n",
       "          (dense): Linear(in_features=2560, out_features=2560, bias=True)\n",
       "        )\n",
       "        (mlp): PhiMLP(\n",
       "          (activation_fn): NewGELUActivation()\n",
       "          (fc1): Linear(in_features=2560, out_features=10240, bias=True)\n",
       "          (fc2): Linear(in_features=10240, out_features=2560, bias=True)\n",
       "        )\n",
       "        (input_layernorm): LayerNorm((2560,), eps=1e-05, elementwise_affine=True)\n",
       "        (resid_dropout): Dropout(p=0.1, inplace=False)\n",
       "      )\n",
       "    )\n",
       "    (rotary_emb): PhiRotaryEmbedding()\n",
       "    (embed_dropout): Dropout(p=0.0, inplace=False)\n",
       "    (final_layernorm): LayerNorm((2560,), eps=1e-05, elementwise_affine=True)\n",
       "  )\n",
       "  (lm_head): Linear(in_features=2560, out_features=51200, bias=True)\n",
       ")"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# âœ… Load the Phi-2 model\n",
    "model_id = \"microsoft/phi-2\"\n",
    "device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
    "\n",
    "tokenizer = AutoTokenizer.from_pretrained(model_id)\n",
    "model = AutoModelForCausalLM.from_pretrained(model_id, torch_dtype=torch.float16 if device==\"cuda\" else torch.float32)\n",
    "model.to(device)\n",
    "model.eval()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5b635bc4",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "llm_rag_env",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
